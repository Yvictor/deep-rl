{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.3.0', '2.0.8')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "import gym\n",
    "import cv2\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "tf.__version__, keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from io import BytesIO\n",
    "def to_png(a):\n",
    "    with BytesIO() as bio:\n",
    "        Image.fromarray(a).save(bio, 'png')\n",
    "        return bio.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! rm -rf log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENV_ID = 'Pong-v0'\n",
    "RESIZE_WIDTH, RESIZE_HEIGHT = (84, 84)\n",
    "AGENT_HISTORY_LENGTH = 4 # the nomber of most recent fraames experienced by the agent that are given as input to the Q nn\n",
    "ACTION_REPEAT = 1\n",
    "γ = 0.99 # discount factor \n",
    "INIT_ϵ = 1.\n",
    "FINAL_ϵ = .1\n",
    "FINAL_ϵ_FRAME = 1000000\n",
    "REPLAY_START_SIZE = 20000 #50000 paper original\n",
    "REPLAY_MEMORY_SIZE = 500000 #1000000 paper original\n",
    "BATCH_SIZE = 32\n",
    "TARGET_NET_UPDATE_FREQ = 10000\n",
    "UPDATE_FREQ = 4\n",
    "LEARNING_RATE = 0.00025 \n",
    "GRAD_MOMENTUM = 0.95\n",
    "SQUARED_GRAD_MOMENTUM = 0.95\n",
    "MIN_SQUARED_GRAD = 0.01\n",
    "NO_OP_MAX = 30\n",
    "NUM_EVAL = 30 # The trained agents were evaluated by playing each game 30 times\n",
    "\n",
    "SAVE_FREQ = 100\n",
    "TRAINING = True\n",
    "SAVE_NN_PATH = 'nn/%s'%ENV_ID\n",
    "SAVE_LOG_PATH = 'log/%s'%ENV_ID\n",
    "\n",
    "if not os.path.exists(SAVE_NN_PATH):\n",
    "    os.makedirs(SAVE_NN_PATH)\n",
    "\n",
    "if not os.path.exists(SAVE_LOG_PATH):\n",
    "    os.makedirs(SAVE_LOG_PATH)\n",
    "\n",
    "EPISODES = 15000  # Number of episodes the agent plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN_Agent:\n",
    "    # iniitialize the Deep Q Learning Agent \n",
    "    def __init__(self, env, restore=False, episode=None):\n",
    "        self.action_dim = env.action_space.n # action number\n",
    "        self.ϵ = INIT_ϵ\n",
    "        self.ϵ_step = ((INIT_ϵ - FINAL_ϵ) / FINAL_ϵ_FRAME)*ACTION_REPEAT\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "        self.T = 0\n",
    "        \n",
    "        # Create q network\n",
    "        self.s, self.q_values, q_network = self.create_Q_network(nn_name='q')\n",
    "        q_network_weights = q_network.trainable_weights\n",
    "\n",
    "        # Create target network\n",
    "        self.st, self.target_q_values, target_network = self.create_Q_network(nn_name='target')\n",
    "        target_network_weights = target_network.trainable_weights\n",
    "        \n",
    "        # Create target network update operation\n",
    "        self.update_target_network = [target_network_weight.assign(q_network_weights[i]) for i, target_network_weight in enumerate(target_network_weights)]\n",
    "        \n",
    "        # Create loss func and gradient descent operation\n",
    "        self.act, self.y, self.loss, self.grad_des = self.loss_function(q_network_weights)\n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        \n",
    "        # user saver to save q_network weights\n",
    "        self.saver = tf.train.Saver(q_network_weights, max_to_keep=0)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        # Initialize target network weights with q network weights\n",
    "        self.sess.run(self.update_target_network)\n",
    "        \n",
    "        if restore:\n",
    "            self.restore_network(episode)\n",
    "        \n",
    "        # logging\n",
    "        self.total_reward = 0\n",
    "        self.total_q_max = 0\n",
    "        self.total_loss = 0\n",
    "        self.duration = 0\n",
    "        self.episode = 0\n",
    "        \n",
    "        self.summary_placeholders, self.update_ops, self.summary_op = self.setup_summary()\n",
    "        self.summary_writer = tf.summary.FileWriter(SAVE_LOG_PATH, self.sess.graph)\n",
    "        \n",
    "    # create DQN network\n",
    "    def create_Q_network(self, nn_name):\n",
    "        input_state = Input(shape=(RESIZE_WIDTH, RESIZE_HEIGHT, AGENT_HISTORY_LENGTH), dtype='float32', name='inputs_%s'%nn_name)\n",
    "        conv = Conv2D(filters=32, kernel_size=8, strides=(4,4), activation='relu', name='conv1_%s'%nn_name)(input_state)\n",
    "        conv = Conv2D(filters=64, kernel_size=4, strides=(2,2), activation='relu', name='conv2_%s'%nn_name)(conv)\n",
    "        conv = Conv2D(filters=64, kernel_size=3, strides=(1,1), activation='relu', name='conv3_%s'%nn_name)(conv)\n",
    "        flat = Flatten(name='flatten_%s'%nn_name)(conv)\n",
    "        fc = Dense(512, activation='relu',  name='fc1_%s'%nn_name)(flat)\n",
    "        Q_pred = Dense(self.action_dim, name='q_pred_%s'%nn_name)(fc)\n",
    "\n",
    "        model = Model(inputs=[input_state], outputs=[Q_pred], name=nn_name)\n",
    "        return input_state, Q_pred, model\n",
    "        \n",
    "    \n",
    "    # DQN loss function\n",
    "    def loss_function(self, q_network_weights):\n",
    "        with tf.name_scope('action'):\n",
    "            act = tf.placeholder(tf.int64, [None])\n",
    "        with tf.name_scope('y'):\n",
    "            y = tf.placeholder(tf.float32, [None])\n",
    "        with tf.name_scope('action_one_hot'):\n",
    "            act_one_hot = tf.one_hot(act, self.action_dim, 1.0, 0.0)\n",
    "        with tf.name_scope('q_action'):\n",
    "            q_action = tf.reduce_sum(tf.multiply(self.q_values, act_one_hot),\n",
    "                                     reduction_indices=1)\n",
    "        with tf.name_scope('loss_function'):\n",
    "            # loss = tf.reduce_mean(tf.square(y-q_action))\n",
    "            # error clipping further improved the stability of the algorithm\n",
    "            error = tf.abs(y - q_action)\n",
    "            quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
    "            linear_part = error - quadratic_part\n",
    "            loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
    "        with tf.name_scope('RMSprop'):\n",
    "            optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, \n",
    "                                                  momentum=GRAD_MOMENTUM, \n",
    "                                                  epsilon=MIN_SQUARED_GRAD)\n",
    "        with tf.name_scope('minimize_loss'):\n",
    "            gradient_descent = optimizer.minimize(loss, var_list=q_network_weights)\n",
    "        return act, y, loss, gradient_descent\n",
    "    \n",
    "    # DQN skill stack frame\n",
    "    def preprocessing_init_state(self, state, previous_state):\n",
    "        preprocessing_state = self.preprocessing(state, previous_state)\n",
    "        ϕ = np.repeat(preprocessing_state, AGENT_HISTORY_LENGTH, axis=-1)\n",
    "        return ϕ\n",
    "        \n",
    "    # DQN skill replay memory buffer\n",
    "    def experience_replay(self, ϕt, action, reward, gg, state_t):\n",
    "        #ϕt1 = ϕt.copy()\n",
    "        #ϕt1[:,:,:-1] = ϕt1[:,:,1:]\n",
    "        #ϕt1[:,:,-1] = state_t[:,:,0]\n",
    "        ϕt1 = np.append(ϕt[:, :, 1:], state_t, axis=2)\n",
    "        \n",
    "        # Clipping the rewards in this manner limits the scale of the error derivatives and\n",
    "        # makes it easier to use the same learning rate across multiple games.\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "        \n",
    "        # Store transition (ϕt, at, rt, ϕt+1) in replay memory but we need to store whether gg to calculate j\n",
    "        self.replay_memory.append((ϕt, action, reward, ϕt1, gg))\n",
    "        \n",
    "        # for statics\n",
    "        self.total_reward += reward\n",
    "        if self.duration % ACTION_REPEAT == 0:\n",
    "            self.total_q_max += np.max(self.q_values.eval(feed_dict={self.s: [ϕt.astype(np.float32)/255.]}))\n",
    "        self.duration += 1\n",
    "        \n",
    "        if gg:\n",
    "            self.episode += 1\n",
    "            if self.T >= REPLAY_START_SIZE:\n",
    "                statics = [self.total_reward, self.total_q_max/self.duration/ACTION_REPEAT, \n",
    "                           self.duration, self.total_loss/self.duration/UPDATE_FREQ, self.ϵ]\n",
    "                for i, var in enumerate(statics):\n",
    "                    self.sess.run(self.update_ops[i], feed_dict={self.summary_placeholders[i]: var})\n",
    "                summary_str = self.sess.run(self.summary_op)\n",
    "                self.summary_writer.add_summary(summary_str, self.episode)\n",
    "            \n",
    "            self.total_reward = 0\n",
    "            self.total_q_max = 0\n",
    "            self.total_loss = 0\n",
    "            self.duration = 0\n",
    "            \n",
    "        return ϕt1\n",
    "    \n",
    "    def preprocessing_ϕ(self, ϕt, state_t):\n",
    "        return np.append(ϕt[:, :, 1:], state_t, axis=2)\n",
    "\n",
    "    # define to train DNQ Agent\n",
    "    def training(self):\n",
    "        if self.T >= REPLAY_START_SIZE:\n",
    "            # Train network\n",
    "            if self.T % UPDATE_FREQ == 0:\n",
    "                # Sample random minibatch of transition from replay memory buffer\n",
    "                minibatch = np.array(random.sample(self.replay_memory, BATCH_SIZE))\n",
    "                ϕt_batch = flat_obj_array(minibatch[:,0], dtype=np.float32)/255.\n",
    "                action_batch = minibatch[:,1].astype(np.int64)\n",
    "                reward_batch = minibatch[:,2].astype(np.float32)\n",
    "                ϕt1_batch = flat_obj_array(minibatch[:,3], dtype=np.float32)/255.\n",
    "                # our y = r + γmax_aQ^(ϕ_1, a; θ`) when not gg if gg y = r so we convert gg True->0 False->1 \n",
    "                # so we can simply multiply it to get y\n",
    "                gg_batch = 1. - minibatch[:,4].astype(np.float32)\n",
    "\n",
    "                target_q_values_batch = self.target_q_values.eval(feed_dict={self.st: ϕt1_batch})\n",
    "                y_batch = reward_batch + γ*np.max(target_q_values_batch, axis=1)*gg_batch\n",
    "\n",
    "                loss, _ = self.sess.run([self.loss, self.grad_des], \n",
    "                                          feed_dict={\n",
    "                                                    self.s: ϕt_batch,\n",
    "                                                    self.act: action_batch,\n",
    "                                                    self.y: y_batch\n",
    "                                                    })\n",
    "                # log loss\n",
    "                self.total_loss += loss\n",
    "                \n",
    "\n",
    "            # Update target network\n",
    "            if self.T % TARGET_NET_UPDATE_FREQ == 0:\n",
    "                self.sess.run(self.update_target_network)\n",
    "                \n",
    "            # Save DQN session to restore\n",
    "            self.save_dqn_sess()\n",
    "                \n",
    "            \n",
    "        \n",
    "        self.T += 1\n",
    "    \n",
    "    def save_dqn_sess(self):\n",
    "        if self.episode % SAVE_FREQ == 0 and self.duration == 0:\n",
    "            chkp_path = self.saver.save(self.sess, os.path.join(SAVE_NN_PATH, ENV_ID), global_step=self.episode)\n",
    "            # print('Saved DQN in {}'.format(chkp_path)\n",
    "            \n",
    "    # ϵ-greedy action\n",
    "    def ϵ_greedy_action(self, ϕ):\n",
    "        if np.random.random() <= self.ϵ and self.T < REPLAY_START_SIZE:\n",
    "            action = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            action = np.argmax(self.q_values.eval(feed_dict={self.s: [ϕ.astype(np.float32)/255.]}))\n",
    "        \n",
    "        if self.ϵ > FINAL_ε and self.T >= REPLAY_START_SIZE:\n",
    "            self.ϵ -= self.ϵ_step\n",
    "        return action\n",
    "    \n",
    "    # action when test\n",
    "    def action_at_test(self, ϕ):\n",
    "        # according to original paper it can avoid the overfiting with set ϵ=0.05\n",
    "        if np.random.random() <= 0.05:\n",
    "            action = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            action = np.argmax(self.q_values.eval(feed_dict={self.s: [ϕ.astype(np.float32)/255.]}))\n",
    "        self.T += 1 ## not sure whether testing need to count\n",
    "        return action\n",
    "    \n",
    "    # directly output the action from Q network\n",
    "    def action(self, ϕ):\n",
    "        return np.argmax(self.q_values.eval(feed_dict={self.s: [ϕ.astype(np.float32)/255.]}))\n",
    "        \n",
    "    # DQN skill preprocess the input images\n",
    "    def preprocessing(self, state, previous_state):\n",
    "        encode_frame = np.maximum(state, previous_state)\n",
    "        extract_Y_channel = cv2.cvtColor(encode_frame, cv2.COLOR_RGB2YUV)[:,:,0]\n",
    "        resize_frame = cv2.resize((extract_Y_channel), (RESIZE_WIDTH, RESIZE_HEIGHT))\n",
    "        return resize_frame[:,:,None]\n",
    "    \n",
    "    def restore_network(self, episode):\n",
    "        meta_path = '{}-{}.meta'.format(os.path.join(SAVE_NN_PATH, ENV_ID), episode)\n",
    "        #self.saver = tf.train.import_meta_graph(meta_path, clear_devices=True)\n",
    "        self.saver.restore(self.sess, save_path='{}-{}'.format(os.path.join(SAVE_NN_PATH, ENV_ID), episode))\n",
    "        print(\"Restore DQN episode:{}\".format(episode))      \n",
    "        \n",
    "    def setup_summary(self):\n",
    "        episode_total_reward = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_ID + '/Episode_Reward', episode_total_reward)\n",
    "        episode_avg_max_q = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_ID + '/Episode_Max_Q', episode_avg_max_q)\n",
    "        episode_duration = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_ID + '/Episode_Frame_Count', episode_duration)\n",
    "        episode_avg_loss = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_ID + '/Episode_Loss', episode_avg_loss)\n",
    "        ϵ_tf = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_ID + '/Epsilon', ϵ_tf)\n",
    "        summary_vars = [episode_total_reward, episode_avg_max_q, episode_duration, episode_avg_loss, ϵ_tf]\n",
    "        summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n",
    "        update_ops = [summ_var.assign(summary_placeholders[i]) for i, summ_var in enumerate(summary_vars)]\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        return summary_placeholders, update_ops, summary_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flat_obj_array(arr, dtype=np.float32):\n",
    "    return np.array([a for a in arr], dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7df25854355498ab95970f8772d4034"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_box = ipywidgets.Box([ipywidgets.Image()])\n",
    "display_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(ENV_ID)\n",
    "agent = DQN_Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a0d070eddd414095f07dd3c3126063"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SHOW_TRAINING = True\n",
    "\n",
    "# training\n",
    "for _ in tqdm.tqdm_notebook(range(50)):\n",
    "    gg = False\n",
    "    act_repeat_count = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    if SHOW_TRAINING:\n",
    "        display_box.children[0].value = to_png(state)\n",
    "    for _ in range(np.random.randint(1, NO_OP_MAX)):\n",
    "        previous_state = state\n",
    "        state, reward, gg, info = env.step(0)\n",
    "        if SHOW_TRAINING:\n",
    "            display_box.children[0].value = to_png(state)\n",
    "    ϕ = agent.preprocessing_init_state(state, previous_state)\n",
    "    while not gg:\n",
    "        previous_state = state\n",
    "        if act_repeat_count % ACTION_REPEAT == 0:\n",
    "            action = agent.ϵ_greedy_action(ϕ)\n",
    "            if SHOW_TRAINING:\n",
    "                display_box.children[0].value = to_png(state)\n",
    "        act_repeat_count += 1\n",
    "        state, reward, gg, info = env.step(action)\n",
    "        preprocessing_state = agent.preprocessing(state, previous_state)\n",
    "        ϕ = agent.experience_replay(ϕ, action, reward, gg,  preprocessing_state)\n",
    "        agent.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from nn/Pong-v0/Pong-v0-20\n",
      "Restore DQN episode:20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab37bad2b3d4a859181a54143aac757"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## testing\n",
    "agent.restore_network(episode=20)\n",
    "for _ in tqdm.tqdm_notebook(range(30)):\n",
    "    gg = False\n",
    "    act_repeat_count = 0\n",
    "    state = env.reset()\n",
    "    display_box.children[0].value = to_png(state)\n",
    "\n",
    "    previous_state = state\n",
    "    state, reward, gg, info = env.step(0)\n",
    "    ϕ = agent.preprocessing_init_state(state, previous_state)\n",
    "    \n",
    "    while not gg:\n",
    "        previous_state = state\n",
    "        if act_repeat_count % ACTION_REPEAT == 0:\n",
    "            action = agent.action(ϕ)\n",
    "            display_box.children[0].value = to_png(state)\n",
    "        act_repeat_count += 1\n",
    "        state, reward, gg, info = env.step(action)\n",
    "        preprocessing_state = agent.preprocessing(state, previous_state)\n",
    "        ϕ = agent.preprocessing_φ(ϕ, preprocessing_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
